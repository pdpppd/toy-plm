# Toy Protein Masked Language Model (MLM)

A minimal BERT-style Transformer implementation for protein sequence masked language modeling.

## Features

- ðŸ§¬ **Protein-focused**: 20 canonical amino acids + special tokens
- ðŸŽ­ **BERT-style masking**: Dynamic 15% masking with 80/10/10 strategy
- ðŸ”¥ **Modern PyTorch**: Transformer encoder with mixed precision training
- ðŸ“Š **Comprehensive**: Training, evaluation, and inference utilities
- âš¡ **Efficient**: ~3M parameters, trains on CPU/GPU

## Quick Start

### Requirements

```bash
pip install torch pyyaml tqdm
```

### Training

```bash
# Full training
python -m protein_mlm.train --config configs/default.yaml

# Debug mode (overfits on 10 sequences)
python -m protein_mlm.train --config configs/default.yaml --debug_overfit

# Custom parameters
python -m protein_mlm.train --config configs/default.yaml --epochs 5 --lr 0.0001
```

### Evaluation

```bash
python -m protein_mlm.evaluate --checkpoint checkpoints/large_mlm/best_model.pt
```

### Inference

```bash
# Interactive mode
python -m protein_mlm.inference --checkpoint checkpoints/large_mlm/best_model.pt --interactive

# Single prediction
python -m protein_mlm.inference --checkpoint checkpoints/large_mlm/best_model.pt \
  --sequence "MKTAYIAK[MASK]RQISFVKSHFS"
```

## Architecture

- **Model**: 8-layer Transformer encoder (512d, 8 heads, ~25M parameters)
- **Vocab**: 25 tokens (20 AAs + PAD/CLS/SEP/MASK/UNK)  
- **Sequences**: Up to 1024 residues (including CLS/SEP tokens)
- **Training**: AdamW optimizer with warmup + linear decay
- **Masking**: 15% tokens masked using BERT strategy

## Project Structure

```
toy-plm/
â”œâ”€â”€ protein_mlm/          # Core implementation
â”‚   â”œâ”€â”€ vocab.py          # Vocabulary and tokenization  
â”‚   â”œâ”€â”€ fasta.py          # FASTA loading and preprocessing
â”‚   â”œâ”€â”€ masking.py        # BERT-style dynamic masking
â”‚   â”œâ”€â”€ dataset.py        # PyTorch Dataset and DataLoader
â”‚   â”œâ”€â”€ model.py          # Transformer encoder architecture
â”‚   â”œâ”€â”€ train.py          # Training script and loop
â”‚   â”œâ”€â”€ evaluate.py       # Model evaluation utilities
â”‚   â””â”€â”€ inference.py      # Masked residue prediction
â”œâ”€â”€ configs/              # YAML configurations
â”œâ”€â”€ scripts/              # Shell scripts for training/eval
â”œâ”€â”€ data/                 # FASTA sequences
â”œâ”€â”€ checkpoints/          # Saved models
â””â”€â”€ logs/                 # Training logs
```

## Configuration

Edit `configs/default.yaml` to customize:

- **Model size**: `d_model`, `n_layers`, `n_heads`
- **Training**: `batch_size`, `lr`, `epochs`
- **Data**: `max_len`, `train_split`, `limit`
- **Masking**: `prob`, `mask_ratio`, `random_ratio`

## Example Results

After training, you should see:

- **Loss**: ~4.0 â†’ ~2.0 (depending on data)
- **Perplexity**: ~8-15 on validation
- **Masked accuracy**: ~40-60%

Sample predictions:
```
Position 8: true=K top5=[K:0.34, R:0.18, N:0.07, Q:0.06, E:0.05]
```

## Implementation Notes

- **Apple Silicon optimized**: Auto-detects and uses MPS backend on M1/M2 Macs
- **Device auto-detection**: `device: auto` automatically picks best available (MPS > CUDA > CPU)
- Supports mixed precision training on CUDA (`amp: true`)
- Automatic checkpoint saving and resuming
- Dynamic padding for variable sequence lengths
- Gradient clipping and warmup scheduling
- Modern PyTorch APIs (torch.amp instead of deprecated cuda.amp)

## Extensions

Easy to extend with:
- Larger models (increase `d_model`, `n_layers`)
- Different masking strategies (span masking, etc.)
- Multi-task training (secondary structure, etc.)
- Different tokenization schemes

## License

MIT License - feel free to use for research and education!
